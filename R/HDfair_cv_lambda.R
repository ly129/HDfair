#' HDfair_cv_lambda: Cross-Validation for Optimal Lambda
#'
#' Performs K-fold cross-validation over a sequence of \eqn{\lambda} values to select an optimal
#' tuning parameter for HDfair, using warm-started solution paths from \code{HDfair_sp_lambda}.
#'
#' @param X Numeric matrix of dimension \eqn{n \times p}; the design matrix.
#' @param y Numeric vector of length \eqn{n}; the response variable.
#' @param ma Integer matrix of dimension \eqn{n \times 2}; first column is source indicator (1,2,...), second column is group indicator (1,2,...).
#' @param lambda_length Integer; number of \eqn{\lambda} values to evaluate when \code{lambda_seq} is NULL.
#' @param lambda_ratio Numeric scalar; ratio between the smallest and largest \eqn{\lambda} (\eqn{\lambda_{min}/\lambda_{max}}) when auto-generating the sequence.
#' @param nfold Integer; number of CV folds.
#' @param foldid Integer vector of length \eqn{n}; pre-specified fold assignments. If NULL, folds are generated by stratifying on source-group combinations.
#' @param eta Numeric scalar; fairness-penalty parameter (fixed across folds).
#' @param rho Numeric scalar; augmented Lagrangian parameter for optimization (fixed).
#' @param weighted Logical; if TRUE, use the weighted loss (sum of mean losses per source-group) in CV error calculation; otherwise, use unweighted sum of squared errors.
#' @param adj Numeric scalar; multiplier adjustment factor (passed to \code{HDfair}).
#' @param eps Numeric scalar; convergence tolerance for HDfair iterations.
#' @param maxiter Integer; maximum number of iterations for HDfair.
#' @param verbose Logical; if TRUE, prints progress during CV.
#'
#' @return An object of class \code{"HDfair_cv"}, which is a list containing:
#' \describe{
#'   \item{lambda}{Numeric vector of evaluated \eqn{\lambda} values.}
#'   \item{cvm}{Numeric vector of mean CV error for each \eqn{\lambda}.}
#'   \item{cvsd}{Numeric vector of standard error of CV error.}
#'   \item{cvup}{Numeric vector of \code{cvm + cvsd}.}
#'   \item{cvlo}{Numeric vector of \code{cvm - cvsd}.}
#'   \item{nzero}{Integer array of dimension \eqn{M \times A \times L} where \eqn{L} is \code{lambda_length}; number of nonzero coefficients per source-group and \eqn{\lambda}.}
#'   \item{lambda.min}{Selected \eqn{\lambda} with minimum \code{cvm}.}
#'   \item{lambda.1se}{Largest \eqn{\lambda} within one standard error of the minimum \code{cvm}.}
#'   \item{index}{Named integer vector with indices \code{lam.min} and \code{lam.1se}.}
#'   \item{foldid}{Integer vector of length \eqn{n}; fold assignments used.}
#'   \item{sp}{Result of \code{HDfair_sp_lambda} on full data (solution path).}
#' }
#' @export
HDfair_cv_lambda <- function(
    X,
    y,
    ma,
    lambda_length,
    lambda_ratio,
    nfold,
    foldid = NULL,
    eta,
    rho,
    weighted = FALSE,
    adj=1,
    eps=1e-6,
    maxiter=1e4,
    verbose = FALSE
) {
  N = nrow(X)
  p = ncol(X)
  M = max(ma[,1])
  A = max(ma[,2])

  thetas <- array(dim = c(p, A, M, lambda_length))
  g <- matrix(nrow = A, ncol = lambda_length)
  iters <- integer(lambda_length)

  ### Initial solution path
  sp <- HDfair_sp_lambda(
    X = X,
    y = y,
    ma = ma,
    lambda_length = lambda_length,
    lambda_ratio = lambda_ratio,
    eta = eta,
    rho = rho,
    weighted = weighted,
    adj=adj,
    eps=eps,
    maxiter=maxiter,
    verbose = verbose
  )
  nzero <- array(dim = c(M, A, lambda_length))
  for (m in 1:M) {
    for (a in 1:A) {
      nzero[m, a, ] <- rowSums(apply(sp$estimates[, a, m, ], MARGIN = 1, "!=", 0))
    }
  }

  lam.seq <- sp$lambdas

  ### train-test split and pre-allocate result storage
  if (is.null(foldid)) {
    grp <- with(as.data.frame(ma), paste(V1, V2, sep = "_"))
    fold_id <- integer(nrow(ma))

    for (g in unique(grp)) {
      idx <- which(grp == g)
      n   <- length(idx)
      # assign 1:K in a recycled way, then shuffle
      fold_id[idx] <- sample(rep(seq_len(nfold), length.out = n))
    }
  } else {
    fold_id <- foldid
  }

  loss_mat <- matrix(0, nrow = nfold, ncol = lambda_length)

  ### cv
  for (i in 1:nfold) {
    which_train <- fold_id != i
    which_val <- fold_id == i

    train_x <- X[which_train, ]
    train_y <- y[which_train]
    train_ma <- ma[which_train, ]
    val_x <- X[which_val, ]
    val_y <- y[which_val]
    val_ma <- ma[which_val, ]

    sp_fold <- HDfair_sp_lambda(
      X = train_x,
      y = train_y,
      ma = train_ma,
      lambda_seq = lam.seq,
      eta = eta,
      rho = rho,
      weighted = weighted,
      adj=adj,
      eps=eps,
      maxiter=maxiter,
      verbose = verbose
    )

    for (m in 1:M) {
      for (a in 1:A) {
        index_ma <- val_ma[, 1] == m & val_ma[, 2] == a
        val_xma <- val_x[index_ma, ]
        val_yma <- val_y[index_ma]
        for (l in 1:lambda_length) {
          th_mal <- sp_fold$estimates[, a, m, l]
          loss_mat[i, l] <- loss_mat[i, l] + sum((val_yma - val_xma %*% th_mal)^2)/sum(which_val)
        }
      }
    }
  }
  if (weighted) loss_mat <- loss_mat/M/A

  ### cv evaluation
  cvm <- apply(loss_mat, MARGIN = 2, FUN = mean)
  cvsd <- apply(loss_mat, MARGIN = 2, FUN = sd)/sqrt(nfold)
  cvup <- cvm + cvsd
  cvlo <- cvm - cvsd
  lam.min <- which.min(cvm)
  lam.1se <- sum(cvm[seq(lam.min)] > cvup[lam.min]) + 1



  # ylims <- range(c(cvup, cvlo))
  # plot(x = log(lam.seq), y = cvm, ylim = ylims, col = 0,
  #      ylab = "CV Error",
  #      xlab = expression(paste("Log(", lambda, ")")))
  # for (i in 1:nlam) {
  #   segments(x0 = log(lam.seq)[i], y0 = cvup[i],
  #            x1 = log(lam.seq)[i], y1 = cvlo[i],
  #            col = 'grey60')
  # }
  # points(x = log(lam.seq), y = cvup, pch = 95, cex = 1, col = 'grey60')
  # points(x = log(lam.seq), y = cvlo, pch = 95, cex = 1, col = 'grey60')
  # points(x = log(lam.seq), y = cvm, pch = 20, cex = 1, col = 'red')
  # abline(v = log(lam.seq[lam.min]), lty = 3)
  # abline(v = log(lam.seq[lam.1se]), lty = 3)
  # axis(side = 3, at = log(lam.seq), labels = nzero, tick = FALSE)

  return.obj <- list(lambda = lam.seq,
                     cvm = cvm,
                     cvsd = cvsd,
                     cvup = cvup,
                     cvlo = cvlo,
                     nzero = nzero,
                     lambda.min = lam.seq[lam.min],
                     lambda.1se = lam.seq[lam.1se],
                     index = c(lam.min = lam.min, lam.1se = lam.1se),
                     foldid = fold_id,
                     sp = sp)
  class(return.obj) <- "HDfair_cv"
  return(return.obj)
}



#' plot_cv_lambda: Plot Cross-Validation Error vs Lambda
#'
#' Plots mean CV error with error bars across the log of \eqn{\lambda} sequence and
#' marks the optimal \code{lambda.min} and \code{lambda.1se}.
#'
#' @param cv_lambda Object of class \code{"HDfair_cv"} returned by \code{HDfair_cv_lambda}.
#'
#' @return NULL. Generates a plot on the current graphics device.
#' @export

plot_cv_lambda <- function(cv_lambda) {
  cvup <- cv_lambda$cvup
  cvlo <- cv_lambda$cvlo
  cvm <- cv_lambda$cvm

  lam.seq <- cv_lambda$lambda
  lambda.min <- cv_lambda$lambda.min
  lambda.1se <- cv_lambda$lambda.1se

  ylims <- range(c(cvup, cvlo))
  plot(x = log(lam.seq), y = cvm, ylim = ylims, col = 0,
       ylab = "CV Error",
       xlab = expression(paste("Log(", lambda, ")")))
  for (i in 1:length(cvm)) {
    segments(x0 = log(lam.seq)[i], y0 = cvup[i],
             x1 = log(lam.seq)[i], y1 = cvlo[i],
             col = 'grey60')
  }
  points(x = log(lam.seq), y = cvup, pch = 95, cex = 1, col = 'grey60')
  points(x = log(lam.seq), y = cvlo, pch = 95, cex = 1, col = 'grey60')
  points(x = log(lam.seq), y = cvm, pch = 20, cex = 1, col = 'red')
  abline(v = log(lambda.min), lty = 3)
  abline(v = log(lambda.1se), lty = 3)

  axis(side = 3,
       at = log(lam.seq),
       labels = apply(cv_lambda$nzero, MARGIN = 3, FUN = mean),
       tick = FALSE)
}
